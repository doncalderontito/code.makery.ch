{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mxnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doncalderontito/code.makery.ch/blob/master/htr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if_HR7Sq4kZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e2182f52-4305-4855-9cb9-db19d43c6ebf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzZeEiWPLQ3_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d028516a-fc1d-4e05-8723-a60cb7a61943"
      },
      "source": [
        "!git clone https://github.com/doncalderontito/htr2.git content/drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'content/drive'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Total 57 (delta 0), reused 0 (delta 0), pack-reused 57\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n",
            "Checking out files: 100% (51/51), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rlpbf62LR-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "005ab790-5210-46ce-8ae9-409d4f9babe4"
      },
      "source": [
        "!pip install -r /content/content/drive/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/27/07b57d22496ed6c98b247e578712122402487f5c265ec70a747900f97060/gluonnlp-0.9.1.tar.gz (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 3.2MB/s \n",
            "\u001b[?25hCollecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
            "\u001b[K     |████████████████████████████████| 68.7MB 57kB/s \n",
            "\u001b[?25hCollecting leven\n",
            "  Downloading https://files.pythonhosted.org/packages/73/02/37084115516cfd595ee2f9a873fffe8b85c6b1538523ff6a8b8dd7ff7d46/leven-1.0.4.tar.gz\n",
            "Collecting mxboard\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/e4/57f6884c39b471c8fd446dc59998045ceab1c9ebe4a6091c953d97a60934/mxboard-0.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 45.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp->-r /content/content/drive/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp->-r /content/content/drive/requirements.txt (line 1)) (0.29.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp->-r /content/content/drive/requirements.txt (line 1)) (20.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r /content/content/drive/requirements.txt (line 2)) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from leven->-r /content/content/drive/requirements.txt (line 3)) (1.12.0)\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from mxboard->-r /content/content/drive/requirements.txt (line 4)) (7.0.0)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from mxboard->-r /content/content/drive/requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses->-r /content/content/drive/requirements.txt (line 6)) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->-r /content/content/drive/requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->-r /content/content/drive/requirements.txt (line 6)) (0.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses->-r /content/content/drive/requirements.txt (line 6)) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp->-r /content/content/drive/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r /content/content/drive/requirements.txt (line 2)) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r /content/content/drive/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r /content/content/drive/requirements.txt (line 2)) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r /content/content/drive/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.0.0->mxboard->-r /content/content/drive/requirements.txt (line 4)) (47.3.1)\n",
            "Building wheels for collected packages: gluonnlp, leven, nltk, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp36-cp36m-linux_x86_64.whl size=470053 sha256=b59d125f4351970a0c46a4a6448a3ef60c8a98adc93243ff5bcc99bd435ef05e\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/60/16/1f8a40e68b85bd9bd7960e91830bca5e40cd113f3220b7e231\n",
            "  Building wheel for leven (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for leven: filename=leven-1.0.4-cp36-cp36m-linux_x86_64.whl size=54676 sha256=c2d38d8744ed239e707932bcf13600e1fef5524d9fda17e1e37df00f71f76526\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/64/a5/439db671d666a50f3b3cebd2dcab3fbbab02785adf58e47552\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449904 sha256=53ea2a3253295787072d172b4d5f0e80af0fb99cc620c5649097c26ac278db44\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=e9e6124b9eda91290bcb8aac31f44e4b896506f2d298d58a66a893ef3dadbce0\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp leven nltk sacremoses\n",
            "Installing collected packages: gluonnlp, graphviz, mxnet, nose, leven, mxboard, nltk, sacremoses\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed gluonnlp-0.9.1 graphviz-0.8.4 leven-1.0.4 mxboard-0.1.0 mxnet-1.6.0 nltk-3.4.5 nose-1.3.7 sacremoses-0.0.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsoNFpRzratZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import difflib\n",
        "import importlib\n",
        "import math\n",
        "import cv2 as cv2\n",
        "import numpy as np\n",
        "import mxnet as mx\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import gluonnlp as nlp\n",
        "import leven\n",
        "import matplotlib.patches as patches\n",
        "from skimage import transform as skimage_tf, exposure\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKsB4fc9AclK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from content.drive.ocr.utils.expand_bounding_box import expand_bounding_box"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GMODVjlrfJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from content.drive.ocr.utils.sclite_helper import ScliteHelper\n",
        "from content.drive.ocr.utils.word_to_line import sort_bbs_line_by_line, crop_line_images\n",
        "from content.drive.ocr.utils.iam_dataset import IAMDataset, resize_image, crop_image, crop_handwriting_page\n",
        "from content.drive.ocr.utils.encoder_decoder import Denoiser, ALPHABET, encode_char, decode_char, EOS, BOS\n",
        "from content.drive.ocr.utils.beam_search import ctcBeamSearch\n",
        "\n",
        "import content.drive.ocr.utils.denoiser_utils\n",
        "import content.drive.ocr.utils.beam_search\n",
        "\n",
        "importlib.reload(content.drive.ocr.utils.denoiser_utils)\n",
        "from content.drive.ocr.utils.denoiser_utils import SequenceGenerator\n",
        "\n",
        "importlib.reload(content.drive.ocr.utils.beam_search)\n",
        "from content.drive.ocr.utils.beam_search import ctcBeamSearch\n",
        "\n",
        "\n",
        "\n",
        "from content.drive.ocr.paragraph_segmentation_dcnn import SegmentationNetwork, paragraph_segmentation_transform\n",
        "from content.drive.ocr.word_and_line_segmentation import SSD as WordSegmentationNet, predict_bounding_boxes\n",
        "from content.drive.ocr.handwriting_line_recognition import Network as HandwritingRecognitionNet, handwriting_recognition_transform\n",
        "from content.drive.ocr.handwriting_line_recognition import decode as decoder_handwriting, alphabet_encoding\n",
        "ctx = mx.gpu(0) if mx.context.num_gpus() > 0 else mx.cpu()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmeP2gp4AYeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIpdGo1KriR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0FT_KyL0z6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper functions\n",
        "def resize_image(image, desired_size):\n",
        "    ''' Helper function to resize an image while keeping the aspect ratio.\n",
        "    Parameter\n",
        "    ---------\n",
        "\n",
        "    image: np.array\n",
        "        The image to be resized.\n",
        "\n",
        "    desired_size: (int, int)\n",
        "        The (height, width) of the resized image\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "\n",
        "    image: np.array\n",
        "        The image of size = desired_size\n",
        "\n",
        "    bounding box: (int, int, int, int)\n",
        "        (x, y, w, h) in percentages of the resized image of the original\n",
        "    '''\n",
        "    size = image.shape[:2]\n",
        "    if size[0] > desired_size[0] or size[1] > desired_size[1]:\n",
        "        ratio_w = float(desired_size[0]) / size[0]\n",
        "        ratio_h = float(desired_size[1]) / size[1]\n",
        "        ratio = min(ratio_w, ratio_h)\n",
        "        new_size = tuple([int(x * ratio) for x in size])\n",
        "        image = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "        size = image.shape\n",
        "\n",
        "    delta_w = max(0, desired_size[1] - size[1])\n",
        "    delta_h = max(0, desired_size[0] - size[0])\n",
        "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "\n",
        "    color = image[0][0]\n",
        "    if color < 230:\n",
        "        color = 230\n",
        "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=float(color))\n",
        "    crop_bb = (left / image.shape[1], top / image.shape[0], (image.shape[1] - right - left) / image.shape[1],\n",
        "               (image.shape[0] - bottom - top) / image.shape[0])\n",
        "    image[image > 230] = 255\n",
        "    return image, crop_bb\n",
        "\n",
        "\n",
        "# this function takes in the img file path\n",
        "def _pre_process_image(img_in, _parse_method):\n",
        "    im = cv2.imread(img_in, cv2.IMREAD_GRAYSCALE)\n",
        "    if np.size(im) == 1:  # skip if the image data is corrupt.\n",
        "        return None\n",
        "    # reduce the size of form images so that it can fit in memory.\n",
        "    if _parse_method in [\"form\", \"form_bb\"]:\n",
        "        im, _ = resize_image(im, MAX_IMAGE_SIZE_FORM)\n",
        "    if _parse_method == \"line\":\n",
        "        im, _ = resize_image(im, MAX_IMAGE_SIZE_LINE)\n",
        "    if _parse_method == \"word\":\n",
        "        im, _ = resize_image(im, MAX_IMAGE_SIZE_WORD)\n",
        "    img_arr = np.asarray(im)\n",
        "    return img_arr\n",
        "\n",
        "\n",
        "def get_arg_max(prob):\n",
        "    '''\n",
        "    The greedy algorithm convert the output of the handwriting recognition network\n",
        "    into strings.\n",
        "    '''\n",
        "    arg_max = prob.topk(axis=2).asnumpy()\n",
        "    return decoder_handwriting(arg_max)[0]\n",
        "\n",
        "\n",
        "def get_beam_search(prob, width=5):\n",
        "    possibilities = ctcBeamSearch(prob.softmax()[0].asnumpy(), alphabet_encoding, None, width)\n",
        "    return possibilities[0]\n",
        "\n",
        "\n",
        "def get_denoised(prob, ctc_bs=False):\n",
        "    if ctc_bs:  # Using ctc beam search before denoising yields only limited improvements a is very slow\n",
        "        text = get_beam_search(prob)\n",
        "    else:\n",
        "        text = get_arg_max(prob)\n",
        "    src_seq, src_valid_length = encode_char(text)\n",
        "    src_seq = mx.nd.array([src_seq], ctx=ctx)\n",
        "    src_valid_length = mx.nd.array(src_valid_length, ctx=ctx)\n",
        "    encoder_outputs, _ = denoiser.encode(src_seq, valid_length=src_valid_length)\n",
        "    states = denoiser.decoder.init_state_from_encoder(encoder_outputs,\n",
        "                                                      encoder_valid_length=src_valid_length)\n",
        "    inputs = mx.nd.full(shape=(1,), ctx=src_seq.context, dtype=np.float32, val=BOS)\n",
        "    output = generator.generate_sequences(inputs, states, text)\n",
        "    return output.strip()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6fZANHqJq7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load \n",
        "paragraph_segmentation_net = SegmentationNetwork(ctx=ctx)\n",
        "paragraph_segmentation_net.cnn.load_parameters(\"content/drive/models/paragraph_segmentation2.params\", ctx=ctx)\n",
        "paragraph_segmentation_net.hybridize()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMh-1KubB8Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_segmentation_net = WordSegmentationNet(2, ctx=ctx)\n",
        "word_segmentation_net.load_parameters(\"content/drive/models/word_segmentation2.params\")\n",
        "word_segmentation_net.hybridize()\n",
        "\n",
        "\n",
        "handwriting_line_recognition_net = HandwritingRecognitionNet(rnn_hidden_states=512,\n",
        "                                                            rnn_layers=2, ctx=ctx, max_seq_len=160)\n",
        "handwriting_line_recognition_net.load_parameters(\"content/drive/models/handwriting_line8.params\", ctx=ctx)\n",
        "handwriting_line_recognition_net.hybridize()\n",
        "\n",
        "FEATURE_LEN = 150\n",
        "denoiser = Denoiser(alphabet_size=len(ALPHABET), max_src_length=FEATURE_LEN, max_tgt_length=FEATURE_LEN, num_heads=16, embed_size=256, num_layers=2)\n",
        "denoiser.load_parameters('content/drive/models/denoiser2.params', ctx=ctx)\n",
        "\n",
        "denoiser.hybridize(static_alloc=True)\n",
        "\n",
        "ctx_nlp = mx.cpu(0)\n",
        "language_model, vocab = nlp.model.big_rnn_lm_2048_512(dataset_name='gbw', pretrained=True,ctx=ctx_nlp)\n",
        "moses_tokenizer = nlp.data.SacreMosesTokenizer()\n",
        "moses_detokenizer = nlp.data.SacreMosesDetokenizer()\n",
        "\n",
        "beam_sampler = nlp.model.BeamSearchSampler(beam_size=20,\n",
        "                                           decoder=denoiser.decode_logprob,\n",
        "                                           eos_id=EOS,\n",
        "                                           scorer=nlp.model.BeamSearchScorer(),\n",
        "                                           max_length=150)\n",
        "\n",
        "\n",
        "generator = SequenceGenerator(beam_sampler, language_model, vocab, ctx_nlp, moses_tokenizer, moses_detokenizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdGYk4JrAJ2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_op(img_n,folder_path):\n",
        "  image_name = img_n.split('.')[0]\n",
        "  img_path = os.path.join(img_dir, img_n)\n",
        "  image = _pre_process_image(img_path, 'form')\n",
        "\n",
        "  form_size = (1120, 800)\n",
        "\n",
        "\n",
        "  predicted_bbs = []\n",
        "\n",
        "\n",
        "  resized_image = paragraph_segmentation_transform(image, form_size)\n",
        "  bb_predicted = paragraph_segmentation_net(resized_image.as_in_context(ctx))\n",
        "  bb_predicted = bb_predicted[0].asnumpy()\n",
        "  bb_predicted = expand_bounding_box(bb_predicted, expand_bb_scale_x=0.03,\n",
        "                                    expand_bb_scale_y=0.03)\n",
        "  predicted_bbs.append(bb_predicted)\n",
        "\n",
        "  (x, y, w, h) = bb_predicted\n",
        "  image_h, image_w = image.shape[-2:]\n",
        "  (x, y, w, h) = (x * image_w, y * image_h, w * image_w, h * image_h)\n",
        "  \n",
        "  segmented_paragraph_size = (700, 700)\n",
        "  paragraph_segmented_images = []\n",
        "\n",
        "  bb = predicted_bbs[0]\n",
        "  image = crop_handwriting_page(image, bb, image_size=segmented_paragraph_size)\n",
        "  paragraph_segmented_images.append(image)\n",
        "  \n",
        "\n",
        "  min_c = 0.1\n",
        "  overlap_thres = 0.1\n",
        "  topk = 600\n",
        "  predicted_words_bbs_array = []\n",
        "\n",
        "  for i, paragraph_segmented_image in enumerate(paragraph_segmented_images):\n",
        "      predicted_bb = predict_bounding_boxes(\n",
        "          word_segmentation_net, paragraph_segmented_image, min_c, overlap_thres, topk, ctx)\n",
        "\n",
        "      predicted_words_bbs_array.append(predicted_bb)\n",
        "      for j in range(predicted_bb.shape[0]):\n",
        "          (x, y, w, h) = predicted_bb[j]\n",
        "          image_h, image_w = paragraph_segmented_image.shape[-2:]\n",
        "          (x, y, w, h) = (x * image_w, y * image_h, w * image_w, h * image_h)\n",
        "\n",
        "  line_images_array = []\n",
        "\n",
        "  for i, paragraph_segmented_image in enumerate(paragraph_segmented_images):\n",
        "      predicted_bbs = predicted_words_bbs_array[i]\n",
        "      line_bbs = sort_bbs_line_by_line(predicted_bbs, y_overlap=0.4)\n",
        "      line_images = crop_line_images(paragraph_segmented_image, line_bbs)\n",
        "      line_images_array.append(line_images)\n",
        "\n",
        "      for line_bb in line_bbs:\n",
        "          (x, y, w, h) = line_bb\n",
        "          image_h, image_w = paragraph_segmented_image.shape[-2:]\n",
        "          (x, y, w, h) = (x * image_w, y * image_h, w * image_w, h * image_h)\n",
        "\n",
        "\n",
        "\n",
        "  line_image_size = (60, 800)\n",
        "  character_probs = []\n",
        "  for line_images in line_images_array:\n",
        "      form_character_prob = []\n",
        "      for i, line_image in enumerate(line_images):\n",
        "          line_image = handwriting_recognition_transform(line_image, line_image_size)\n",
        "          line_character_prob = handwriting_line_recognition_net(line_image.as_in_context(ctx))\n",
        "          form_character_prob.append(line_character_prob)\n",
        "      character_probs.append(form_character_prob)\n",
        "\n",
        "\n",
        "  FEATURE_LEN = 150\n",
        "  save_path = os.path.join(folder_path, image_name+'.txt')\n",
        "  file = open(save_path, 'w')\n",
        "\n",
        "  for i, form_character_probs in enumerate(character_probs):\n",
        "    for j, line_character_probs in enumerate(form_character_probs):\n",
        "        decoded_line_bs = get_beam_search(line_character_probs)\n",
        "        print(decoded_line_bs)\n",
        "        file.write(decoded_line_bs + ' ')\n",
        "  file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zERTmWu0zyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main code\n",
        "\n",
        "MAX_IMAGE_SIZE_FORM = (1120, 800)\n",
        "MAX_IMAGE_SIZE_LINE = (60, 800)\n",
        "MAX_IMAGE_SIZE_WORD = (30, 140)\n",
        "\n",
        "img_dir = \"/content/content/drive/small/\"\n",
        "img_names = os.listdir(img_dir)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUT5UhsWMnWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa3f2f1b-b80e-449e-8fcb-425ec8d55e30"
      },
      "source": [
        "img_names "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['testimage2.jpeg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6upfE0HUMhKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_path = os.path.abspath('/content/content/drive/output')\n",
        "if(not os.path.exists(folder_path)):\n",
        "  print(\"creating dir\")\n",
        "  os.mkdir(folder_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7x7unmyTxLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "5673480e-43f0-470b-bf7e-288da746a602"
      },
      "source": [
        "for img_n in img_names[:4]:\n",
        "  print(img_n)\n",
        "  print(\"\\n New image starting\\n\")\n",
        "  generate_op(img_n,folder_path)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testimage2.jpeg\n",
            "\n",
            " New image starting\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-beca99cc0a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n New image starting\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mgenerate_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_op' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP9LHNYy0eA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-2IiYtTMtjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}